<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Research on Decoding AI&#39;s Evolution</title>
    <link>https://amanpriyanshu.github.io/blogs/categories/ai-research/</link>
    <description>Recent content in AI Research on Decoding AI&#39;s Evolution</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright> </copyright>
    <lastBuildDate>Wed, 06 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://amanpriyanshu.github.io/blogs/categories/ai-research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Contra-Topic-bottleneck-t5: Efficient Topic Extraction Without the Computational Overhead</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/contra-topic/</link>
      <pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/contra-topic/</guid>
      <description>Quick Links: Model on HuggingFace | Interactive Demo
When it comes to topic extraction, the AI world seems fixated on massive models and expensive compute. But what if there was a simpler way? ü§î
The Genesis: Simplicity Through Linear Transformation Picture this: There I was, looking for an open-source solution to extract topics from text at scale. The available options were either massive language models or complex fine-tuning pipelines. That&amp;rsquo;s when it hit me ‚Äì what if we could leverage the semantic structure of existing embeddings with just a linear transformation?</description>
    </item>
    
    <item>
      <title>LinearCosine: When AI Researchers Decided Multiplication was Too Mainstream</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/linear-cosine/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/linear-cosine/</guid>
      <description>Hey there, optimization seekers and efficiency enthusiasts! üìäüßÆ Today, we&amp;rsquo;re diving into a world where even basic arithmetic operations are up for debate. Buckle up as we explore LinearCosine, an experiment that asks: &amp;ldquo;Do we really need multiplication for AI?&amp;rdquo;
Quick Links to skip the talk: Project Website - Linear Cosine | GitHub Repo | Original Paper
The Paper That Started It All During my fall break, while I was supposed to be relaxing, my roommate Yash Maurya forwarded me a fascinating paper by Hongyin Luo and Wei Sun titled &amp;ldquo;Addition is All You Need for Energy-efficient Language Models&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>AdaptKeyBERT: Stumbling Through Two Years of Keyword Extraction</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/adaptkeybert/</link>
      <pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/adaptkeybert/</guid>
      <description>Quick links (in case you want to skip my ramblings):
PyPI Package GitHub Repository
Alright, gather &amp;lsquo;round, word enthusiasts and syntax sorcerers! üßô‚Äç‚ôÇÔ∏èüìö Remember that time you tried to explain machine learning to your grandma and ended up comparing neural networks to her knitting patterns? Well, buckle up, because we&amp;rsquo;re about to dive into a similar realm of &amp;ldquo;What was I thinking?&amp;rdquo; ‚Äì the saga of AdaptKeyBERT.
It&amp;rsquo;s been two trips around the sun since I cobbled together this quirky little keyword extractor and sent it off into the wild world of NLP.</description>
    </item>
    
    <item>
      <title>YC-Dendrolinguistics: Planting Linguistic Trees in the Startup Forest</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/startup-linguistic-trees/</link>
      <pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/startup-linguistic-trees/</guid>
      <description>Hey there, fellow AI adventurers and startup enthusiasts! üå≥üöÄ Today, I&amp;rsquo;m excited to give you a peek into my latest passion project: YC-Dendrolinguistics. Buckle up as we embark on a journey through the linguistic forests of Y-Combinator pitches!
The Seed of an Idea Picture this: It&amp;rsquo;s 2 AM, I&amp;rsquo;m knee-deep in YC application videos, and suddenly it hits me ‚Äì what if startup pitches are like trees? ü§î Each word a branch, each phrase a limb, growing into this complex organism we call a pitch.</description>
    </item>
    
    <item>
      <title>Synaptic Sparks: Why I&#39;m Wiring My Thoughts into a Neural Blogosphere</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/synaptic-sparks/</link>
      <pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/synaptic-sparks/</guid>
      <description>Hey there, fellow AI enthusiasts and curious minds! üß†ü§ñ Today, I just want to document what&amp;rsquo;s leading to this new adventure in regular blogging.
The Knowledge Synapse Picture me back in 2019, a wide-eyed novice bouncing around the vast landscape of machine learning. I was devouring every GitHub gist, Medium post, and arXiv paper I could find, growing and learning at a dizzying pace. Fast forward to today, and it feels like I&amp;rsquo;ve stepped into an alternate universe.</description>
    </item>
    
    <item>
      <title>FRACTURED-SORRY-Bench: Unraveling AI Safety through Decomposing Malicious Intents</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/fractured-sorry-bench/</link>
      <pubDate>Wed, 28 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/fractured-sorry-bench/</guid>
      <description>Hello, fellow AI enthusiasts! ü§ñ Today, I wanted to dive into the FRACTURED-SORRY-Bench framework and dataset we just released. Check out the dataset, website, and github for the dataset!
The FRACTURED-SORRY Saga: A Tale of Adaptation and Decomposition Picture this: you&amp;rsquo;re wandering through the lush collection of prompt-injection and llm-red-teaming papers, marveling at some of the weird and some of the crazier attack mechanisms that have been released recently. When suddenly, you realize that there aren&amp;rsquo;t many Proof-of-Concept resources for multi-shot red-teaming.</description>
    </item>
    
  </channel>
</rss>
