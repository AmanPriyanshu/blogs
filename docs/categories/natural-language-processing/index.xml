<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing on Decoding AI&#39;s Evolution</title>
    <link>https://amanpriyanshu.github.io/blogs/categories/natural-language-processing/</link>
    <description>Recent content in Natural Language Processing on Decoding AI&#39;s Evolution</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright> </copyright>
    <lastBuildDate>Mon, 11 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://amanpriyanshu.github.io/blogs/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Teaching AI to Read and Group Like I Bookmark the Web: A Journey into Dynamic Topic Modeling</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/dynamic-topic-modeling/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/dynamic-topic-modeling/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;
&lt;a href=&#34;https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens&#34;&gt;Dataset on HuggingFace&lt;/a&gt;&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/AmanPriyanshu/blogs/refs/heads/main/content/posts/2024/Dynamic-Topic-Modeling/images/dynamic-topics-banner.png&#34; alt=&#34;Dynamic Topic Modeling&#34; style=&#34;width: 96%;&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;the-topic-modeling-challenge&#34;&gt;The Topic Modeling Challenge&lt;/h2&gt;
&lt;p&gt;You know that feeling when you have 50 browser tabs open, and you&amp;rsquo;re desperately trying to organize them into bookmark folders? &amp;ldquo;ML Papers To Read,&amp;rdquo; &amp;ldquo;Funny Cat Videos,&amp;rdquo; &amp;ldquo;Recipes I&amp;rsquo;ll Never Make&amp;rdquo;&amp;hellip; We all have our system. And apparently, it&amp;rsquo;s such a universal problem that every tech company is launching their own solution - Arc Browser with its &amp;ldquo;Spaces,&amp;rdquo; Chrome with its tab groups, and about 500 extensions promising to color-code your digital hoarding habits into submission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Contra-Topic-bottleneck-t5: Efficient Topic Extraction Without the Computational Overhead</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/contra-topic/</link>
      <pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/contra-topic/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;
&lt;a href=&#34;https://huggingface.co/AmanPriyanshu/Contra-Topic-bottleneck-t5-large&#34;&gt;Model on HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/drive/1_SuTiL3QS-PUYjSrugqqD5mQlMv8Hbfc?usp=sharing&#34;&gt;Interactive Demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When it comes to topic extraction, the AI world seems fixated on massive models and expensive compute. But what if there was a simpler way? ü§î&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/AmanPriyanshu/blogs/refs/heads/main/content/posts/2024/Contra-Topic/images/contra-topic-banner.png&#34; alt=&#34;Topic Modeling&#34; style=&#34;width: 96%;&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;the-genesis-simplicity-through-linear-transformation&#34;&gt;The Genesis: Simplicity Through Linear Transformation&lt;/h2&gt;
&lt;p&gt;Picture this: There I was, looking for an open-source solution to extract topics from text at scale. The available options were either massive language models or complex fine-tuning pipelines. That&amp;rsquo;s when it hit me ‚Äì what if we could leverage the semantic structure of existing embeddings with just a linear transformation?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AdaptKeyBERT: Stumbling Through Two Years of Keyword Extraction</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/adaptkeybert/</link>
      <pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/adaptkeybert/</guid>
      <description>&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://amanpriyanshu.github.io/AdaptKeyBERT/images/adaptkeybert_revisited.png&#34; alt=&#34;Running Demo 1&#34; style=&#34;width: 90%;&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Quick links (in case you want to skip my ramblings):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/adaptkeybert/&#34;&gt;PyPI Package&lt;/a&gt;
&lt;a href=&#34;https://github.com/AmanPriyanshu/AdaptKeyBERT&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Alright, gather &amp;lsquo;round, word enthusiasts and syntax sorcerers! üßô‚Äç‚ôÇÔ∏èüìö Remember that time you tried to explain machine learning to your grandma and ended up comparing neural networks to her knitting patterns? Well, buckle up, because we&amp;rsquo;re about to dive into a similar realm of &amp;ldquo;What was I thinking?&amp;rdquo; ‚Äì the saga of AdaptKeyBERT.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
