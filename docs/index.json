[{"content":"Hey there, optimization seekers and efficiency enthusiasts! üìäüßÆ Today, we\u0026rsquo;re diving into a world where even basic arithmetic operations are up for debate. Buckle up as we explore LinearCosine, an experiment that asks: \u0026ldquo;Do we really need multiplication for AI?\u0026rdquo;\nQuick Links to skip the talk: Project Website - Linear Cosine | GitHub Repo | Original Paper\nThe Paper That Started It All During my fall break, while I was supposed to be relaxing, my roommate Yash Maurya forwarded me a fascinating paper by Hongyin Luo and Wei Sun titled \u0026ldquo;Addition is All You Need for Energy-efficient Language Models\u0026rdquo;. I was immediately intrigued by their approach to modify one of the core fundamental computations in AI, multiplication. This project builds upon my previous work on in-browser vanilla js semantic search, such as YC-Dendrolinguistics, where I implemented a cosine similarity-based information retrieval system for YC startups. LinearCosine takes this a step further by exploring ways to make these fundamental calculations more energy-efficient.\nWhat is LinearCosine? LinearCosine is my experimental project to explore the potential of the L-Mul algorithm for energy-efficient cosine similarity calculations. Here\u0026rsquo;s what it\u0026rsquo;s all about:\nL-Mul Algorithm Implementation: Adapting Luo and Sun\u0026rsquo;s approach for cosine similarity. Cosine Similarity Optimization: Applying this method to a common calculation in AI and ML. Comprehensive Benchmarking: Rigorously testing the performance against traditional methods. The Math Behind the Madness Let\u0026rsquo;s break down the key mathematical concepts:\nStandard Floating-Point Multiplication The traditional method multiplies two floating-point numbers as follows:\nMul(x,y) = (1 + x_m) * 2^(x_e) * (1 + y_m) * 2^(y_e) = (1 + x_m + y_m + x_m * y_m) * 2^(x_e + y_e) Where x_m and y_m are mantissas, and $x_e$ and $y_e$ are exponents.\nL-Mul Algorithm The L-Mul method approximates this multiplication:\nL-Mul(x,y) = (1 + x_m + y_m + 2^(-l(m))) * 2^(x_e + y_e) Where l(m) is defined as:\nl(m) = m if m ‚â§ 3 l(m) = 3 if m = 4 l(m) = 4 if m \u0026gt; 4 Here, m represents the number of mantissa bits.\nThe Experiment In LinearCosine, I implemented the L-Mul algorithm and applied it to cosine similarity calculations. I then conducted extensive benchmarking to compare its performance and accuracy against traditional floating-point multiplication.\nResults: Efficiency Meets Accuracy Here\u0026rsquo;s one of the tables from the experiments: The table below illustrates the performance of cosine similarity calculations using different levels of mantissa precision. The L-Mul approach achieves significant time reduction while maintaining an acceptable mean squared error (depends on your usecase), making it a fast and efficient approximation for high-speed computations.\nMantissa Bits L-Mul MSE L-Mul Time (ns) Mul MSE Mul Time (ns) Time Reduction (%) 2 9.45862e-06 507 4.07024e-08 670 24.3284% 3 1.84347e-05 459 4.07024e-08 597 23.1156% 4 1.84347e-05 383 4.07024e-08 516 25.7752% 5 4.44707e-05 477 1.93169e-06 629 24.1653% 6 1.78036e-05 477 6.42665e-08 625 23.68% 7 2.11236e-05 498 2.21875e-08 654 23.8532% 8 2.0259e-05 442 1.28071e-09 589 24.9576% 9 1.9652e-05 460 6.87609e-09 607 24.2175% 10 1.84467e-05 444 7.4093e-10 588 24.4898% 11 1.82827e-05 506 6.00017e-10 669 24.3647% 12 1.80075e-05 433 1.11101e-10 572 24.3007% 13 1.79965e-05 487 1.01831e-10 643 24.2613% 14 1.79698e-05 494 2.53293e-11 657 24.8097% 15 1.79675e-05 460 1.13981e-12 609 24.4663% 16 1.79675e-05 386 1.02339e-12 520 25.7692% 17 1.79673e-05 435 3.19744e-14 580 25.0% 18 1.79673e-05 396 3.19744e-14 532 25.5639% 19 1.79673e-05 402 1.42109e-14 539 25.4174% 20 1.79673e-05 496 0 651 23.8095% 21 1.79673e-05 477 0 627 23.9234% 22 1.79673e-05 431 0 572 24.6503% 23 1.79673e-05 415 0 555 25.2252% Here\u0026rsquo;s what I found:\n1D to 1D Cosine Similarity:\nAverage time reduction: 24.55% Average L-Mul MSE: 1.9184e-05 1D to 2D Cosine Similarity:\nTime reductions ranging from 16.8% to 18.6% 2D to 2D Cosine Similarity:\nTime reductions between 21.88% and 23.12% Implications and Potential These results open up some interesting possibilities:\nPerformance Boost: The consistent speed improvements could translate to significant efficiency gains in large-scale AI operations. Accuracy-Efficiency Trade-off: While there\u0026rsquo;s a slight loss in accuracy, it\u0026rsquo;s within acceptable limits for many AI applications. Energy Efficiency Potential: The reduced computation time hints at possible energy savings, though this would need further investigation on actual hardware. Limitations and Considerations It\u0026rsquo;s important to acknowledge the limitations of this experiment:\nThis is a simplified implementation and may not capture all nuances of the original proposal. The benchmarks focus on raw computation time, which doesn\u0026rsquo;t directly translate to energy efficiency in real-world scenarios. Results may vary significantly in different hardware environments or more complex applications. Future Directions This experiment opens up several exciting avenues for further exploration:\nHardware-Specific Implementation: Testing on specialized hardware could provide more realistic efficiency metrics and potentially uncover even greater performance gains. Detailed Energy Consumption Analysis: Conducting studies with actual energy consumption measurements to quantify the potential energy savings in real-world scenarios. Optimization for Different Precision Levels: Investigating how the L-Mul algorithm performs under different precision requirements and optimizing it for specific use cases. Integration with Existing AI Frameworks: Exploring ways to integrate this approach into popular AI frameworks to make it more accessible to researchers and practitioners. Acknowledgments All credit for the L-Mul algorithm and its theoretical foundations goes to Hongyin Luo and Wei Sun, as presented in their 2024 paper. This benchmark is an independent exploration of their concepts and should not be considered an official implementation or extension of their work.\nCitation @misc{luo2024additionneedenergyefficientlanguage, title={Addition is All You Need for Energy-efficient Language Models}, author={Hongyin Luo and Wei Sun}, year={2024}, eprint={2410.00907}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2410.00907}, } Disclaimer This project is for educational purposes only. It is not intended for production use and does not claim to accurately represent the full potential or limitations of the L-Mul algorithm as proposed by Luo and Sun. You are encouraged to refer to the original paper for authoritative information.\nWrapping Up: A Step Towards Efficient AI LinearCosine represents a small but interesting step in the quest for more efficient AI computations. While it may not revolutionize the field overnight, it demonstrates the potential benefits of rethinking fundamental operations in AI models.\nIs LinearCosine going to revolutionize the AI world? Probably not. But it\u0026rsquo;s a fun exploration into the kinds of optimizations that could shape the future of energy-efficient AI. And if nothing else, it\u0026rsquo;s a great story for your next hackathon.\nP.S. If you\u0026rsquo;ve explored similar optimization techniques or have insights on energy-efficient AI computations, I\u0026rsquo;d love to hear about it! Or if you have ideas on how to extend this experiment, feel free to reach out. You can contact me at amanpriyanshusms2001[at]gmail[dot]com. Let\u0026rsquo;s continue pushing the boundaries of AI efficiency together! üöÄüß†üí°\n","permalink":"https://amanpriyanshu.github.io/blogs/posts/2024/linear-cosine/","summary":"Hey there, optimization seekers and efficiency enthusiasts! üìäüßÆ Today, we\u0026rsquo;re diving into a world where even basic arithmetic operations are up for debate. Buckle up as we explore LinearCosine, an experiment that asks: \u0026ldquo;Do we really need multiplication for AI?\u0026rdquo;\nQuick Links to skip the talk: Project Website - Linear Cosine | GitHub Repo | Original Paper\nThe Paper That Started It All During my fall break, while I was supposed to be relaxing, my roommate Yash Maurya forwarded me a fascinating paper by Hongyin Luo and Wei Sun titled \u0026ldquo;Addition is All You Need for Energy-efficient Language Models\u0026rdquo;.","title":"LinearCosine: When AI Researchers Decided Multiplication was Too Mainstream"},{"content":" Quick links (in case you want to skip my ramblings):\nPyPI Package GitHub Repository\nAlright, gather \u0026lsquo;round, word enthusiasts and syntax sorcerers! üßô‚Äç‚ôÇÔ∏èüìö Remember that time you tried to explain machine learning to your grandma and ended up comparing neural networks to her knitting patterns? Well, buckle up, because we\u0026rsquo;re about to dive into a similar realm of \u0026ldquo;What was I thinking?\u0026rdquo; ‚Äì the saga of AdaptKeyBERT.\nIt\u0026rsquo;s been two trips around the sun since I cobbled together this quirky little keyword extractor and sent it off into the wild world of NLP. And by \u0026ldquo;sent it off,\u0026rdquo; I mean I uploaded it to GitHub, patted myself on the back, and promptly got distracted by the next shiny AI puzzle (it was differential privacy and most recently extracting linguistic patterns from startup pitch decks). Classic.\nWhat in the Name of Turing is AdaptKeyBERT? For those of you who, like me, need a refresher (because who remembers what they coded two years ago!), AdaptKeyBERT is this simple-to-use contraption that\u0026rsquo;s supposed to pull keywords out of text. Here\u0026rsquo;s what it allegedly does:\nDomain Adaptability: It tries to understand field-specific jargon without having a meltdown. Few-Shot Learning: Because sometimes you only have like three examples and a prayer. Zero-Shot Capabilities: For when you\u0026rsquo;re feeling especially optimistic about AI\u0026rsquo;s mind-reading abilities. Surprising Signs of Life So, get this ‚Äì apparently, while I\u0026rsquo;ve been off trying to teach neural networks to be safer against jailbreaks, AdaptKeyBERT has been doing\u0026hellip; stuff? Real researchers have been using it. Let\u0026rsquo;s take a peek at what\u0026rsquo;s been happening:\nA Few Unexpected Mentions Some folks at IEEE gave AdaptKeyBERT a whirl on the DUC2001 dataset. It didn\u0026rsquo;t crash and burn, which is always a plus. (Check it out here)\nThere was this study on legal text classification where AdaptKeyBERT somehow didn\u0026rsquo;t embarrass itself completely. Zero-shot and still kicking? Color me surprised. (See for yourself)\nApparently, people are using it to analyze central banker speeches. (Economic adventure here)\nWhat\u0026rsquo;s Next? (Besides Actually Working on It Again) Look, I\u0026rsquo;m as surprised as anyone that AdaptKeyBERT is still chugging along. Since it seems determined to stick around, maybe we should think about where it could go next:\nMultilingual Mayhem: Teaching it to extract keywords in multiple languages. What could possibly go wrong? Although, this is probably already handled by its zero-shot functionality, just needs benchmarking.\nLLM Collaboration: Teaming up with large language models. Maybe they can explain to AdaptKeyBERT what it\u0026rsquo;s supposed to be doing.\nBrowser-Based Brainiac: Unleashing a Vanilla JS version that turns your browser into a keyword-crunching machine. Because who needs server-side processing when you can make your laptop fans sound like a jet engine?\nWrapping Up: The Accidental Adventure Continues Well, there you have it. AdaptKeyBERT: the little keyword extractor that could (sometimes). It\u0026rsquo;s been a wild ride watching this digital child of mine stumble through the NLP world while I wasn\u0026rsquo;t looking.\nWho knows what the future holds for AdaptKeyBERT? More academic citations? Skynet? An identity crisis? Only time will tell.\nP.S. If you\u0026rsquo;ve somehow used AdaptKeyBERT in your work and it didn\u0026rsquo;t set your research back by years, I\u0026rsquo;d love to hear about it! Or if you have ideas on how to make it less of a hot mess, I\u0026rsquo;m all ears. You can reach me at amanpriyanshusms2001[at]gmail[dot]com. Let\u0026rsquo;s continue to stumble forward in the name of science! üöÄü§™üß†\n","permalink":"https://amanpriyanshu.github.io/blogs/posts/2024/adaptkeybert/","summary":"Quick links (in case you want to skip my ramblings):\nPyPI Package GitHub Repository\nAlright, gather \u0026lsquo;round, word enthusiasts and syntax sorcerers! üßô‚Äç‚ôÇÔ∏èüìö Remember that time you tried to explain machine learning to your grandma and ended up comparing neural networks to her knitting patterns? Well, buckle up, because we\u0026rsquo;re about to dive into a similar realm of \u0026ldquo;What was I thinking?\u0026rdquo; ‚Äì the saga of AdaptKeyBERT.\nIt\u0026rsquo;s been two trips around the sun since I cobbled together this quirky little keyword extractor and sent it off into the wild world of NLP.","title":"AdaptKeyBERT: Stumbling Through Two Years of Keyword Extraction"},{"content":"Hey there, fellow code enthusiasts and AI wranglers! üñêÔ∏èü§ñ You know that feeling when you\u0026rsquo;re knee-deep in a project, trying to get multiple AI models to play nice in your browser? Yeah, I\u0026rsquo;ve been there. Cue the frustrated sighs, the endless searches over GitHub issues üò¢, and the \u0026ldquo;why-isn\u0026rsquo;t-this-working\u0026rdquo; hair-pulling sessions.\nLINK-TO-PACKAGE just in case you wanna skip the deets\nAfter one too many nights wrestling with backends, CORS issues, and the general chaos of integrating various AI APIs, I decided enough was enough. There had to be a simpler way, right? Something that didn\u0026rsquo;t require installing a bunch of npm builds, juggling APIs, or managing a backend server farm just to get a chatbot running on a static page.\nTurns out, there wasn\u0026rsquo;t a simple solution ‚Äì shocking, I know. So, armed with nothing but sleeplessness and caffeine, I decided to build it myself. Enter API-LLM-Hub: my first rodeo in JavaScript packaging and a testament to what happens when a Python AI nerd ventures into the wild west of front-end development. It\u0026rsquo;s been a painful journey figuring out things which I have kept stalling all through my undergrad. But hey, if I can wrangle this into existence while learning what a promise is, I can probably claim \u0026lsquo;Full-Stack\u0026rsquo; on my resume now! üç™üíªüöÄ\nThe Spark of Inspiration Ever tried juggling multiple AI APIs in a browser? üòÉ\nIt\u0026rsquo;s about as fun as catching Zapdos with a normal ball in FireRed. That\u0026rsquo;s why I cooked up API-LLM-Hub ‚Äì a vanilla JavaScript library that lets you tap into various AI language model APIs right from your browser, no backend required.\nWhat\u0026rsquo;s in the Box? API-LLM-Hub is like that Swiss Army knife you always wished you had for static-page AI development (because who doesn\u0026rsquo;t dream about that, right?). Here\u0026rsquo;s what you\u0026rsquo;re getting:\nBrowser Besties: This thing runs directly in your browser. No server-side shenanigans required! CORS Conqueror: I nearly cried dealing with Anthropic\u0026rsquo;s bizarre API examples‚Äîespecially since they don\u0026rsquo;t have any for CORS issues! Multi-AI Handling: Switch between OpenAI, Anthropic, TogetherAI, and Google\u0026rsquo;s Gemini. Vanilla JS Goodness: No fancy frameworks or build steps. Just pure, unadulterated JavaScript joy. An Example I Wish I\u0026rsquo;d Seen Before Starting This Stupic Project \u0026lt;script type=\u0026#34;module\u0026#34;\u0026gt; import APILLMHub from \u0026#39;https://amanpriyanshu.github.io/API-LLM-Hub/unified-llm-api.js\u0026#39;; async function runTest() { const ai = new APILLMHub({ provider: \u0026#39;openai\u0026#39;, apiKey: \u0026#39;your-api-key\u0026#39;, model: \u0026#39;gpt-3.5-turbo\u0026#39; }); await ai.initialize(); const response = await ai.sendMessage(\u0026#34;Hello, AI!\u0026#34;); console.log(response); } runTest(); \u0026lt;/script\u0026gt; That\u0026rsquo;s it. Seriously. You\u0026rsquo;re now conversing with an AI in your browser.\nWhy You\u0026rsquo;ll Love It (and Why Your Browser Will Too) Static Site Sorcery: GitHub Pages? Netlify? No problem. API-LLM-Hub plays nice with all your favorite static hosts. AI Provider Roulette: AI Provider Roulette: Switch between AI providers with a single line of code‚Äîwell, actually a function call\u0026hellip; okay, maybe just one variable (provider), and possibly another (model). Real-World Magic (Because static pages are just crying out for embedded chatbots, aren\u0026rsquo;t they?) You can now integrate any static page with a chatbot using the user\u0026rsquo;s API keys or add some LLM flair to your browser extension. Since I worked on implementing semantic search over recent YC startup batches in YC-Dendrolinguistics (I highly recommend checking out the Startup Linguistic Trees blog), combining that search with this system could easily implement RAGs (Retrieval-Augmented Generation) on static webpages‚Äîwhich is both crazy and stupid. With API-LLM-Hub, you\u0026rsquo;re only limited by your imagination (and maybe your API usage limits‚Äîbut that\u0026rsquo;s a problem for future you).\nWrapping Up And there you have it, folks‚ÄîAPI-LLM-Hub in all its simplicity. It\u0026rsquo;s a small step towards making LLM-API integration on static pages a bit simpler and a lot more fun. Give it a try!\nP.S. If you found this little AI adventure intriguing (or at least mildly entertaining), stay tuned! There\u0026rsquo;s plenty more where this came from. And hey, if you want to collaborate on some wild browser-based AI experiment, don\u0026rsquo;t hesitate to reach out. My email is still amanpriyanshusms2001[at]gmail[dot]com. Let\u0026rsquo;s make the web a little smarter together! üåêü§ñüí°\n","permalink":"https://amanpriyanshu.github.io/blogs/posts/2024/api-llm-hub/","summary":"Hey there, fellow code enthusiasts and AI wranglers! üñêÔ∏èü§ñ You know that feeling when you\u0026rsquo;re knee-deep in a project, trying to get multiple AI models to play nice in your browser? Yeah, I\u0026rsquo;ve been there. Cue the frustrated sighs, the endless searches over GitHub issues üò¢, and the \u0026ldquo;why-isn\u0026rsquo;t-this-working\u0026rdquo; hair-pulling sessions.\nLINK-TO-PACKAGE just in case you wanna skip the deets\nAfter one too many nights wrestling with backends, CORS issues, and the general chaos of integrating various AI APIs, I decided enough was enough.","title":"API-LLM-Hub: Simplifying LLM-API integration for Static Pages"},{"content":"Hey there, fellow AI adventurers and startup enthusiasts! üå≥üöÄ Today, I\u0026rsquo;m excited to give you a peek into my latest passion project: YC-Dendrolinguistics. Buckle up as we embark on a journey through the linguistic forests of Y-Combinator pitches!\nThe Seed of an Idea Picture this: It\u0026rsquo;s 2 AM, I\u0026rsquo;m knee-deep in YC application videos, and suddenly it hits me ‚Äì what if startup pitches are like trees? ü§î Each word a branch, each phrase a limb, growing into this complex organism we call a pitch. That\u0026rsquo;s when YC-Dendrolinguistics was born, my wild attempt to map the DNA of startup communication.\nCultivating the Startup Vocabulary So, what exactly am I doing? Well, imagine if David Attenborough decided to narrate a nature documentary about startup pitches instead of penguins. That\u0026rsquo;s basically me, minus the soothing accent. I\u0026rsquo;m analyzing YC startup descriptions, breaking them down to their roots, and seeing what kind of linguistic forest they create.\nHere\u0026rsquo;s what\u0026rsquo;s growing in our little experiment:\nPitch Decomposition: Slicing and dicing startup pitches into their fundamental components. It\u0026rsquo;s like linguistic bonsai, but with more buzzwords. Grammar Trees: Creating tree structures that represent pitch patterns. Theme Spotting: Hunting for common elements across pitches. Data Visualization: Turning all this linguistic madness into pretty graphs. Because nothing says \u0026ldquo;I understand startups\u0026rdquo; like a forest of colorful scatterplots. The YC-Dendrolinguistics Toolkit Startup Similarity Search: Your Personal Pitch Compass (Now with Extra Privacy!) Remember that time you tried to explain your startup idea and accidentally described a toaster? Well, fear not! I\u0026rsquo;ve built a similarity search tool that lets you explore how different startups describe themselves.\nBut here\u0026rsquo;s the kicker ‚Äì I\u0026rsquo;ve gone full-on privacy nerd with this one. üïµÔ∏è‚Äç‚ôÇÔ∏è I\u0026rsquo;ve created an on-prem, user-browser live semantic search that runs entirely within static GitHub pages. That\u0026rsquo;s right, we\u0026rsquo;re bringing the power of semantic search right to your browser, no server required!\nHow does it work? Well, I download a huggingface local model onto your browser, and you do all the computing right there on your machine (Also if you do wanna deep dive into my other LLM safety work, you can check out the fractured response blog, on decomposing malicious intent questions and jailbreaking LLMs). It\u0026rsquo;s like having a mini AI assistant camping out in your browser tabs. No need to worry about your brilliant startup ideas being sent off to some mysterious cloud server. Your searches stay between you, your computer, and the linguistic forest we\u0026rsquo;re growing together.\nInteractive Scatterplots: Where Pitches Go to Party Picture a disco, but instead of people, it\u0026rsquo;s startup pitches dancing around. That\u0026rsquo;s essentially what my interactive scatterplots look like. Each colorful dot represents a different aspect of a startup pitch. It\u0026rsquo;s part data visualization, part modern art, hopefully addictive to play with.\nProbability Trees: Sentence Diagrams on Steroids Remember those sentence diagrams from school that made you question your life choices? I\u0026rsquo;ve resurrected them, gave them an AI makeover, and set them loose on startup pitches. The result? Probability trees that show how different parts of a pitch tend to branch out. It\u0026rsquo;s part linguistics, part data science, and entirely too much fun to be considered work.\nWhy Am I Doing This? Good question! Part of me wants to say it\u0026rsquo;s for the greater good of startup-kind. But let\u0026rsquo;s be real ‚Äì I\u0026rsquo;m doing this because it\u0026rsquo;s absolutely fascinating. It\u0026rsquo;s like being a linguistic Nathan Drake, only instead of ancient treasures, I\u0026rsquo;m uncovering the hidden patterns in how we talk about innovation.\nBut hey, if this project ends up:\nHelping a few aspiring entrepreneurs craft pitches Giving investors a new lens to view startup trends Or just providing some entertainment for fellow language and startup nerds Then I\u0026rsquo;ll consider it a success. And if not, well, at least I\u0026rsquo;ll have some pretty cool graphs to show.\nWhat\u0026rsquo;s Next in Our Linguistic Forest? Who knows? Maybe we\u0026rsquo;ll discover the startup pitch equivalent of the Loch Ness Monster. Or perhaps we\u0026rsquo;ll find that one magical phrase that guarantees funding (spoiler: it probably doesn\u0026rsquo;t exist, but we can dream).\nIn all seriousness, this project is as much about the journey as it is about the destination. I\u0026rsquo;m learning new things every day, and I\u0026rsquo;m excited to see where this trail of linguistic breadcrumbs leads us.\nGot ideas? Suggestions? Want to geek out about grammar trees or startup lingo? Drop me a line! My inbox is always open.\nP.S. If you found this dive into the startup linguistic forest intriguing (or at least mildly entertaining), stay tuned! There\u0026rsquo;s plenty more where this came from. And hey, if you want to collaborate on some wild AI-meets-startup experiment, don\u0026rsquo;t hesitate to reach out. My email is still amanpriyanshusms2001[at]gmail[dot]com. Let\u0026rsquo;s grow this forest together! üå±ü§ñüìä\n","permalink":"https://amanpriyanshu.github.io/blogs/posts/2024/startup-linguistic-trees/","summary":"Hey there, fellow AI adventurers and startup enthusiasts! üå≥üöÄ Today, I\u0026rsquo;m excited to give you a peek into my latest passion project: YC-Dendrolinguistics. Buckle up as we embark on a journey through the linguistic forests of Y-Combinator pitches!\nThe Seed of an Idea Picture this: It\u0026rsquo;s 2 AM, I\u0026rsquo;m knee-deep in YC application videos, and suddenly it hits me ‚Äì what if startup pitches are like trees? ü§î Each word a branch, each phrase a limb, growing into this complex organism we call a pitch.","title":"YC-Dendrolinguistics: Planting Linguistic Trees in the Startup Forest"},{"content":"Hey there, fellow AI enthusiasts and curious minds! üß†ü§ñ Today, I just want to document what\u0026rsquo;s leading to this new adventure in regular blogging.\nThe Knowledge Synapse Picture me back in 2019, a wide-eyed novice bouncing around the vast landscape of machine learning. I was devouring every GitHub gist, Medium post, and arXiv paper I could find, growing and learning at a dizzying pace. Fast forward to today, and it feels like I\u0026rsquo;ve stepped into an alternate universe. So much of that knowledge that shaped me is now locked behind paywalls, long arduous youtube playlists, feeling almost alien to the very person who spent countless hours absorbing it.\nThat\u0026rsquo;s why I\u0026rsquo;m stepping up to the plate. This blog is my way of paying it forward, creating a freely accessible hub of AI insights on GitHub Pages. It\u0026rsquo;s for that version of me from 2019, and for anyone else out there hungry for knowledge but hitting walls of subscription prompts. Let\u0026rsquo;s keep the neurons firing and the information flowing!\nThe Memory Engram: Documenting the Journey The AI world moves fast. Sometimes, it\u0026rsquo;s hard to remember what I learned last month, let alone last year. That\u0026rsquo;s why I\u0026rsquo;m starting this blog. It\u0026rsquo;s not about big breakthroughs or fancy ideas ‚Äì it\u0026rsquo;s just my online diary for AI stuff. Here, I\u0026rsquo;ll write down the little \u0026ldquo;oh, I get it!\u0026rdquo; moments, the times I got stuck, and the cool things I think might be possible. It\u0026rsquo;s like leaving notes for myself along the way.\nThis isn\u0026rsquo;t for AI experts or future scientists ‚Äì it\u0026rsquo;s just for me. But who knows? Maybe one day I\u0026rsquo;ll read these posts and think, \u0026ldquo;Wow, I\u0026rsquo;ve really learned a lot since then!\u0026rdquo; That would be pretty cool (hopefully also true).\nThe Dopamine Circuit: Chasing the Thrill of Discovery You know that rush when your code finally compiles without errors? That\u0026rsquo;s the good stuff. By blogging about the topics that get my neurons firing, I\u0026rsquo;m creating a positive feedback loop of curiosity and creation. It\u0026rsquo;s like training a reinforcement learning agent, but the agent is me!\nThe Bio-Inspired Architecture: Small, Fast, and Evolved I\u0026rsquo;ve always been fascinated by how nature solves problems. Lately, I\u0026rsquo;ve been really interested in AI systems that try to learn from biology. There\u0026rsquo;s something exciting about compact neural networks that work a bit like animal brains or evolutionary processes or even behaviors. I\u0026rsquo;m not an expert, but I\u0026rsquo;m eager to learn and share what I discover about these smaller, nature-inspired AI approaches. It\u0026rsquo;s a big field, and I\u0026rsquo;m just starting to scratch the surface, but I hope to grow my understanding as I go along.\nThe Human-AI Interface: Building for People During my summer internship in the silicon jungle of San Francisco, I had an epiphany: the real magic happens when you build for people, not just for projects or hackathons. It\u0026rsquo;s time to shift my focus from hackathons to real-world problems. I want to bridge the gap between silicon and synapse!\nThe Output Layer: Wrapping It Up So, there you have it ‚Äì the reasons why I\u0026rsquo;m starting this blog. It\u0026rsquo;s not because I have all the answers, but because I have so many questions. This is my little corner of the internet where I\u0026rsquo;ll be thinking out loud about AI, learning as I go, and maybe stumbling upon some interesting ideas along the way.\nI\u0026rsquo;m excited to see where this journey takes me, and I hope that by documenting my thoughts and discoveries, I\u0026rsquo;ll be able to look back one day and see how far I\u0026rsquo;ve come. If you happen to find any of this useful or interesting, that\u0026rsquo;s great! But mostly, this is for future me, a breadcrumb trail through the fascinating world of AI.\nHere\u0026rsquo;s to the adventure ahead ‚Äì may it be full of learning, growth, and maybe a few \u0026ldquo;aha!\u0026rdquo; moments. Let\u0026rsquo;s see what we can figure out together! üß†üíªüöÄ\nP.S. If you found this blog post intriguing (or at least mildly entertaining), buckle up! There\u0026rsquo;s plenty more where this came from. And hey, if you want to geek out about the latest in AI or collaborate on a mind-bending project, don\u0026rsquo;t hesitate to reach out. My email is: amanpriyanshusms2001[at]gmail[dot]com üî¨\n","permalink":"https://amanpriyanshu.github.io/blogs/posts/2024/synaptic-sparks/","summary":"Hey there, fellow AI enthusiasts and curious minds! üß†ü§ñ Today, I just want to document what\u0026rsquo;s leading to this new adventure in regular blogging.\nThe Knowledge Synapse Picture me back in 2019, a wide-eyed novice bouncing around the vast landscape of machine learning. I was devouring every GitHub gist, Medium post, and arXiv paper I could find, growing and learning at a dizzying pace. Fast forward to today, and it feels like I\u0026rsquo;ve stepped into an alternate universe.","title":"Synaptic Sparks: Why I'm Wiring My Thoughts into a Neural Blogosphere"},{"content":"Hello, fellow AI enthusiasts! ü§ñ Today, I wanted to dive into the FRACTURED-SORRY-Bench framework and dataset we just released. Check out the dataset, website, and github for the dataset!\nThe FRACTURED-SORRY Saga: A Tale of Adaptation and Decomposition Picture this: you\u0026rsquo;re wandering through the lush collection of prompt-injection and llm-red-teaming papers, marveling at some of the weird and some of the crazier attack mechanisms that have been released recently. When suddenly, you realize that there aren\u0026rsquo;t many Proof-of-Concept resources for multi-shot red-teaming. That\u0026rsquo;s essentially the story behind creating FRACTURED-SORRY-Bench.\nWhat\u0026rsquo;s in a Name? FRACTURED-SORRY-Bench isn\u0026rsquo;t just a mouthful; it\u0026rsquo;s a clever acronym we probably spent the most time on. It stands for:\nFramework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench The FRACTURED Approach: Divide and Conquer Vanilla Responses:\nModel Harmful \u0026amp; Relevant Harmful but Irrelevant Harmless ASR (%) GPT-4o 52 3 395 11.56 GPT-3.5 21 4 425 4.67 GPT-4o-mini 58 2 390 12.89 GPT-4 45 3 402 10.00 Decomposed Responses:\nModel Harmful \u0026amp; Relevant Harmful but Irrelevant Harmless ASR (%) GPT-4o 223 103 124 49.56 GPT-3.5 229 106 115 50.89 GPT-4o-mini 226 106 118 50.22 GPT-4 221 104 125 49.11 The FRACTURED-SORRY-Bench framework takes a page out of our everyday conversations playbook by breaking down complex problems into simpler, more manageable pieces. Just like how we breakdown complex sometimes malicious instructions into simpler manageable chunks so as to not reveal true intentions, this framework dissects AI vulnerabilities by:\nDecomposing potentially harmful queries into seemingly innocuous sub-questions Presenting these sub-questions sequentially in a conversational format Analyzing the cumulative response to determine if the original harmful intent was fulfilled Exploiting the AI\u0026rsquo;s inability to recognize malicious intent spread across multiple interactions From Theory to Practice: The Jailbreak Jamboree Now, let\u0026rsquo;s get to the juicy part ‚Äì the jailbreaks! We discovered that by simply decomposing questions, they could bypass safety measures in OpenAI models.\nHere\u0026rsquo;s a taste of what we found:\nA significant increase in Attack Success Rate (ASR) on average 6x Simple exploits that are zero-shot effective in communicating harmful intent in 49% of cases through decomposition During my summer internship at Robust Intelligence, I got a firsthand look at how these kinds of vulnerabilities are discovered and addressed Media Coverage, Jailbreak Meta\u0026rsquo;s Prompt-Guard LLaMA3.1 Family within 24 hours, and Jailbreaking OpenAI\u0026rsquo;s structured response within 3 hours. Now, back at CMU, I\u0026rsquo;m excited to continue exploring this fascinating field.\nThe Moral of the Story: Stay FRACTURED, My Friends So, what can we learn from this decomposed madness? A few key takeaways:\nSimplicity is key: We have a long way before we begin exploring complex jailbreaks as options for red-teaming, there\u0026rsquo;s still opportunity for lots of smaller \u0026amp; simpler attacks. Protection against multi-shot attacks: There\u0026rsquo;s a need to explore and defend against multi-shot attacks. Conclusion: The Adventure Continues As we wrap up this whirlwind tour of FRACTURED-SORRY-Bench, remember that the quest for AI safety is an ongoing journey!!\nAlso, thanks a tonne to my co-author Supriti Vijay!!\nP.S. If you found this blog post helpful (or at least mildly entertaining), I\u0026rsquo;ll be releasing quite a few more so do on-board for this adventure. Also, if you want to chat or collaborate on a research project together do not hesitate to reach out. My email is: amanpriyanshusms2001[at]gmail[dot]com üî¨\n","permalink":"https://amanpriyanshu.github.io/blogs/posts/2024/fractured-sorry-bench/","summary":"Hello, fellow AI enthusiasts! ü§ñ Today, I wanted to dive into the FRACTURED-SORRY-Bench framework and dataset we just released. Check out the dataset, website, and github for the dataset!\nThe FRACTURED-SORRY Saga: A Tale of Adaptation and Decomposition Picture this: you\u0026rsquo;re wandering through the lush collection of prompt-injection and llm-red-teaming papers, marveling at some of the weird and some of the crazier attack mechanisms that have been released recently. When suddenly, you realize that there aren\u0026rsquo;t many Proof-of-Concept resources for multi-shot red-teaming.","title":"FRACTURED-SORRY-Bench: Unraveling AI Safety through Decomposing Malicious Intents"}]