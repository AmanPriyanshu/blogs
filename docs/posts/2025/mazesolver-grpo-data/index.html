<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial | Decoding AI&#39;s Evolution</title>
<meta name="keywords" content="Synthetic Data, Spatial Intelligence, Data Generation, Maze Navigation, Curriculum Learning" />
<meta name="description" content="Quick Links:
GitHub Repository | Dataset Sample
Introduction
Spatial reasoning remains a significant challenge for language models, particularly in tasks requiring 2D navigation and visual-spatial understanding. Current approaches typically rely on either training large vision-language models (VLMs) on visual data or using language models to generate training examples through expensive API calls.
This post explores what might be considered an unconventional (and possibly naive) approach: deterministic generation of spatial reasoning data without requiring any LLMs or VLMs in the data creation process. Rather than using models to generate training examples, this experiment algorithmically creates what we hope are realistic learning trajectories that simulate how spatial reasoning competency might develop over time.">
<meta name="author" content="Aman Priyanshu">
<link rel="canonical" href="https://amanpriyanshu.github.io/blogs/posts/2025/mazesolver-grpo-data/" />
<link crossorigin="anonymous" href="/blogs/assets/css/stylesheet.min.c74c39d1136312e61e25287c3a1af7d0ee301dcc95b02e0ccd190cd309047b28.css" integrity="sha256-x0w50RNjEuYeJSh8Ohr30O4wHcyVsC4MzRkM0wkEeyg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/blogs/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js" integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://amanpriyanshu.github.io/blogs/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://amanpriyanshu.github.io/blogs/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://amanpriyanshu.github.io/blogs/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://amanpriyanshu.github.io/blogs/apple-touch-icon.png">
<link rel="mask-icon" href="https://amanpriyanshu.github.io/blogs/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.148.2">
<link rel="alternate" hreflang="en" href="https://amanpriyanshu.github.io/blogs/posts/2025/mazesolver-grpo-data/" />
<head>






</head>

<meta property="og:title" content="Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial" />
<meta property="og:description" content="Quick Links:
GitHub Repository | Dataset Sample
Introduction
Spatial reasoning remains a significant challenge for language models, particularly in tasks requiring 2D navigation and visual-spatial understanding. Current approaches typically rely on either training large vision-language models (VLMs) on visual data or using language models to generate training examples through expensive API calls.
This post explores what might be considered an unconventional (and possibly naive) approach: deterministic generation of spatial reasoning data without requiring any LLMs or VLMs in the data creation process. Rather than using models to generate training examples, this experiment algorithmically creates what we hope are realistic learning trajectories that simulate how spatial reasoning competency might develop over time." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://amanpriyanshu.github.io/blogs/posts/2025/mazesolver-grpo-data/" />
<meta property="og:image" content="https://amanpriyanshu.github.io/blogs/images/maze.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-16T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2025-01-16T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://amanpriyanshu.github.io/blogs/images/maze.png" />
<meta name="twitter:title" content="Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial"/>
<meta name="twitter:description" content="Quick Links:
GitHub Repository | Dataset Sample
Introduction
Spatial reasoning remains a significant challenge for language models, particularly in tasks requiring 2D navigation and visual-spatial understanding. Current approaches typically rely on either training large vision-language models (VLMs) on visual data or using language models to generate training examples through expensive API calls.
This post explores what might be considered an unconventional (and possibly naive) approach: deterministic generation of spatial reasoning data without requiring any LLMs or VLMs in the data creation process. Rather than using models to generate training examples, this experiment algorithmically creates what we hope are realistic learning trajectories that simulate how spatial reasoning competency might develop over time."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://amanpriyanshu.github.io/blogs/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial",
      "item": "https://amanpriyanshu.github.io/blogs/posts/2025/mazesolver-grpo-data/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial",
  "name": "Creating 2D Spatial Reasoning Data Without LLMs \/ VLMs: A Deterministic Curriculum Trial",
  "description": "Quick Links: GitHub Repository | Dataset Sample\nIntroduction Spatial reasoning remains a significant challenge for language models, particularly in tasks requiring 2D navigation and visual-spatial understanding. Current approaches typically rely on either training large vision-language models (VLMs) on visual data or using language models to generate training examples through expensive API calls.\nThis post explores what might be considered an unconventional (and possibly naive) approach: deterministic generation of spatial reasoning data without requiring any LLMs or VLMs in the data creation process. Rather than using models to generate training examples, this experiment algorithmically creates what we hope are realistic learning trajectories that simulate how spatial reasoning competency might develop over time.\n",
  "keywords": [
    "Synthetic Data", "Spatial Intelligence", "Data Generation", "Maze Navigation", "Curriculum Learning"
  ],
  "articleBody": "Quick Links: GitHub Repository | Dataset Sample\nIntroduction Spatial reasoning remains a significant challenge for language models, particularly in tasks requiring 2D navigation and visual-spatial understanding. Current approaches typically rely on either training large vision-language models (VLMs) on visual data or using language models to generate training examples through expensive API calls.\nThis post explores what might be considered an unconventional (and possibly naive) approach: deterministic generation of spatial reasoning data without requiring any LLMs or VLMs in the data creation process. Rather than using models to generate training examples, this experiment algorithmically creates what we hope are realistic learning trajectories that simulate how spatial reasoning competency might develop over time.\nI should note upfront that this approach has obvious limitations. We’re essentially creating synthetic behaviors based on assumptions about how learning progresses, rather than capturing actual model failures and improvements. Whether this translates to meaningful training signal remains an open question that Part 2 of this series will attempt to address.\nNote: This is Part 1 of a two-part series focusing on the data generation methodology. Part 2 will cover model training and evaluation results, assuming they’re worth sharing.\nThe Challenge with Current Approaches Most spatial reasoning datasets suffer from significant limitations, though I should acknowledge that each approach has valid use cases. LLM-generated data requires expensive API costs for large-scale generation, introduces model bias in generated examples, and makes it difficult to control specific failure patterns (though it does capture more naturalistic language patterns than synthetic approaches). Vision-language models create dependencies on complex visual processing pipelines that are computationally expensive, but they do ground reasoning in actual visual understanding rather than tokenized abstractions. Manual annotation approaches are time-intensive and difficult to scale, but they provide the most authentic human reasoning patterns.\nThe approach described here attempts to sidestep these issues, though it likely introduces new problems of its own.\nMethodology Curriculum Framework Design The framework implements five distinct difficulty levels, each representing what I hypothesize might be different stages of spatial reasoning competency. This is admittedly based on intuition rather than empirical study of how models actually learn spatial tasks.\nLevel 1 simulates initial learning with 5% prediction accuracy, 50% invalid move probability, and 5-step spatial errors from actual positions. The strategy focuses on random exploration with high error rates (though real novice behavior might be even more chaotic than this simplified model suggests). Level 2 introduces basic pattern recognition with 15% accuracy and directional bias emerging after turn 3. Level 3 develops intermediate planning capabilities using single waypoint-based navigation with 25% accuracy. Level 4 advances to dual waypoint planning with oscillation detection, achieving 50% accuracy with zero invalid moves in final turns. Level 5 represents expert performance with perfect A* pathfinding, 100% prediction accuracy, and zero spatial errors.\nThe progression assumes a roughly linear improvement in spatial capabilities, which may not reflect how models actually develop these skills. Real learning curves are likely more jagged, with sudden improvements and occasional regressions that this framework doesn’t capture.\nData Generation Process Each level generates conversational exchanges between human (maze state) and assistant (navigation response). The assistant’s reasoning process includes spatial analysis of current position and target direction, multi-step move planning with appropriate error injection, level-appropriate motivation and strategy selection, move validity checking with position updates, and reflection generation analyzing prediction accuracy and learning progress.\nProgressive Distribution The curriculum uses a dynamic distribution across 100,000 training samples:\nSamples 0-10k: 60% Level 1, 20% Level 2, declining higher levels Samples 10k-40k: Gradual shift toward intermediate levels Samples 40k-60k: Balanced distribution favoring Levels 2-3 Samples 60k-80k: Emphasis on Levels 3-4 with emerging Level 5 Samples 80k-100k: 60% Level 5, 40% Level 4, minimal lower levels Implementation Details Maze Environment The system uses a grid-based maze representation with:\nRandomized maze generation using depth-first search Configurable sizes (4x4 to 8x8 grids) Token-based representation: \u003c|row-col|\u003e, \u003c|up_down_wall|\u003e, \u003c|origin|\u003e, \u003c|target|\u003e Guaranteed solvability with optimal path lengths between 10-20 moves Behavior Modeling Rather than capturing actual model outputs, the system generates hypothetical learning behaviors:\n# Example: Level 3 waypoint strategy def generate_waypoint_moves(self, maze, waypoint_metadata, current_target, turn): if current_target == 'midpoint1': target_pos = waypoint_metadata['midpoint1']['position'] else: target_pos = maze.target # Calculate directional bias toward current objective dr = target_pos[0] - current_pos[0] dc = target_pos[1] - current_pos[1] # Generate bias probabilities bias = self._calculate_directional_bias(dr, dc) return maze.do_random_walk( p=bias, num_steps=5, invalid_chance=self.invalid_chances[turn] ) Conversation Structure Each training sample contains:\nInitial maze state (tokenized representation) Multi-turn conversation (up to 5 turns) Progressive reasoning (improving prediction accuracy) Strategic evolution (from random to optimal planning) Success tracking (target achievement metrics) Key Advantages (and Their Limitations) This approach does offer some practical benefits over model-dependent generation. It eliminates the need for reinforcement learning training loops, policy optimization algorithms, large-scale model checkpointing, and extensive hyperparameter tuning. The synthetic generation allows precise control over error patterns and frequencies, strategic complexity progression, learning curve steepness, and failure mode representation. The framework supports unlimited data generation, diverse maze configurations, configurable difficulty progressions, and multi-domain adaptation potential.\nHowever, these advantages come with significant trade-offs. The precision of control might actually be a weakness, afterall real learning is messier and less predictable than these neat progressions suggest. The unlimited data generation capability is only valuable if the generated data actually teaches useful patterns, which remains to be validated. Most importantly, this approach assumes we understand how spatial reasoning develops well enough to simulate it convincingly, which is probably overconfident.\nExperimental Validation Dataset Characteristics Generated dataset contains:\n100,000 complete maze-solving conversations Average 3.8 turns per conversation (range: 1-5) Behavioral Authenticity The contrast between curriculum levels demonstrates realistic learning progression:\nLevel 1 Example - Spatial Reasoning Breakdown:\nPrediction: \"I think I'll end up at \u003c0-0\u003e\" Reality: Ends at \u003c1-1\u003e Moves: \u003c|right|\u003e \u003c|left|\u003e \u003c|right|\u003e \u003c|left|\u003e \u003c|up|\u003e (oscillatory, inefficient) Invalid attempts: Multiple failed \u003c|down|\u003e moves due to walls Strategy: \"Let me map out the surrounding area\" (confused exploration) Level 5 Example - Expert Performance:\nPrediction: \"I think I'll end up at \u003c4-1\u003e\" Reality: \"Perfect! I'm exactly at \u003c4-1\u003e - execution was flawless\" Moves: Optimal A* pathfinding with dual waypoint strategy Invalid attempts: Zero - all moves valid Strategy: \"Looking ahead, I can see position \u003c4-0\u003e is only 3 moves from target\" This progression from chaotic spatial confusion to perfect navigation demonstrates:\nRealistic error patterns decreasing over curriculum levels Natural language variation reflecting competency changes Strategic evolution from random exploration to optimal planning Authentic learning trajectory simulation What Would Probably Be Better A more rigorous approach would likely combine elements from multiple methodologies. Ideally, we’d start with actual model behavior—training models on spatial tasks and carefully analyzing their failure modes, learning progressions, and breakthrough moments. This would provide empirical grounding for curriculum design rather than relying on assumptions.\nIncorporating human learning data would add another valuable dimension. How do humans develop spatial reasoning skills? What kinds of mistakes do they make, and how do those mistakes change over time? Cognitive science research on spatial learning could inform more realistic error patterns and progression rates.\nA hybrid approach might prove most effective: use LLMs to generate diverse initial examples, analyze real model failures to understand authentic confusion patterns, then use deterministic generation to scale up the most informative training scenarios. This would combine the naturalistic language of LLM generation, the authenticity of real model behavior, and the control and scalability of synthetic approaches.\nFor validation, we’d want to compare not just final task performance but also the learning dynamics—do models trained on this synthetic curriculum develop spatial reasoning in similar ways to models trained through direct experience or human demonstration?\nFuture Directions Dataset and Model Release The complete dataset and models trained on this curriculum framework will be released in the coming months, including:\nFull 100,000-sample training dataset with all curriculum levels Detailed methodology documentation and generation code Evaluation benchmarks for spatial reasoning tasks Conclusion This deterministic approach to spatial reasoning data generation represents an experiment in algorithmic curriculum design rather than a definitive solution. While it offers practical advantages over LLM-based or VLM-dependent methods through cost efficiency with no API calls or GPU-intensive model inference required, precise control over learning progression and error patterns, unlimited scalability with consistent quality, and complete transparency in understanding how each training example was created, these benefits may be offset by the artificial nature of the generated learning trajectories.\nThe framework demonstrates that it may be possible to create structured spatial reasoning training data through pure algorithmic generation, but whether this data actually improves model performance remains an empirical question. The approach might work well as a starting point or supplement to other training data, even if it doesn’t fully replace more authentic alternatives.\nThe real test will be whether models trained on this curriculum develop generalizable spatial reasoning skills or simply learn to mimic the specific patterns encoded in the synthetic data. Part 2 will attempt to provide some preliminary answers, though a comprehensive evaluation would require much more extensive experimentation than a single project can provide.\nComing Next: Part 2 of this series will cover training language models on this curriculum dataset and evaluating their spatial reasoning capabilities, assuming the results are interesting enough to warrant sharing.\nThe complete dataset generation framework will be released regardless of training outcomes, as the methodology itself may be of interest even if the results are disappointing. For questions about the approach or early access to the dataset, please reach out through [amanpriyanshu.github.io].\n",
  "wordCount" : "1558",
  "inLanguage": "en",
  "image":"https://amanpriyanshu.github.io/blogs/images/maze.png","datePublished": "2025-01-16T00:00:00Z",
  "dateModified": "2025-01-16T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Aman Priyanshu"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://amanpriyanshu.github.io/blogs/posts/2025/mazesolver-grpo-data/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Decoding AI's Evolution",
    "logo": {
      "@type": "ImageObject",
      "url": "https://amanpriyanshu.github.io/blogs/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://amanpriyanshu.github.io/blogs/" accesskey="h" title="Decoding AI&#39;s Evolution (Alt + H)">Decoding AI&#39;s Evolution</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://amanpriyanshu.github.io/" title="My homepage">
                    <span>My homepage</span>
                </a>
            </li>
            <li>
                <a href="https://amanpriyanshu.github.io/blogs/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://amanpriyanshu.github.io/blogs/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://amanpriyanshu.github.io/My-Personal-Model-Picks/" title="My Personal Model Picks">
                    <span>My Personal Model Picks</span>
                </a>
            </li>
            <li>
                <a href="https://amanpriyanshu.github.io/blogs/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://amanpriyanshu.github.io/blogs/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://amanpriyanshu.github.io/blogs/">Home</a>&nbsp;»&nbsp;<a href="https://amanpriyanshu.github.io/blogs/posts/">Posts</a></div>
    <h1 class="post-title">
      Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial
    </h1>
    <div class="post-meta">January 16, 2025 | 8 min | Aman Priyanshu
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#the-challenge-with-current-approaches" aria-label="The Challenge with Current Approaches">The Challenge with Current Approaches</a></li>
                <li>
                    <a href="#methodology" aria-label="Methodology">Methodology</a><ul>
                        
                <li>
                    <a href="#curriculum-framework-design" aria-label="Curriculum Framework Design">Curriculum Framework Design</a></li>
                <li>
                    <a href="#data-generation-process" aria-label="Data Generation Process">Data Generation Process</a></li>
                <li>
                    <a href="#progressive-distribution" aria-label="Progressive Distribution">Progressive Distribution</a></li></ul>
                </li>
                <li>
                    <a href="#implementation-details" aria-label="Implementation Details">Implementation Details</a><ul>
                        
                <li>
                    <a href="#maze-environment" aria-label="Maze Environment">Maze Environment</a></li>
                <li>
                    <a href="#behavior-modeling" aria-label="Behavior Modeling">Behavior Modeling</a></li>
                <li>
                    <a href="#conversation-structure" aria-label="Conversation Structure">Conversation Structure</a></li></ul>
                </li>
                <li>
                    <a href="#key-advantages-and-their-limitations" aria-label="Key Advantages (and Their Limitations)">Key Advantages (and Their Limitations)</a></li>
                <li>
                    <a href="#experimental-validation" aria-label="Experimental Validation">Experimental Validation</a><ul>
                        
                <li>
                    <a href="#dataset-characteristics" aria-label="Dataset Characteristics">Dataset Characteristics</a></li>
                <li>
                    <a href="#behavioral-authenticity" aria-label="Behavioral Authenticity">Behavioral Authenticity</a></li></ul>
                </li>
                <li>
                    <a href="#what-would-probably-be-better" aria-label="What Would Probably Be Better">What Would Probably Be Better</a></li>
                <li>
                    <a href="#future-directions" aria-label="Future Directions">Future Directions</a><ul>
                        
                <li>
                    <a href="#dataset-and-model-release" aria-label="Dataset and Model Release">Dataset and Model Release</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Quick Links:</strong>
<a href="https://github.com/AmanPriyanshu/Tiny-Maze-Mock-GRPO">GitHub Repository</a> | <a href="https://huggingface.co/datasets/AmanPriyanshu/Tiny-Maze-Mock-GRPO">Dataset Sample</a></p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Spatial reasoning remains a significant challenge for language models, particularly in tasks requiring 2D navigation and visual-spatial understanding. Current approaches typically rely on either training large vision-language models (VLMs) on visual data or using language models to generate training examples through expensive API calls.</p>
<p>This post explores what might be considered an unconventional (and possibly naive) approach: <strong>deterministic generation of spatial reasoning data</strong> without requiring any LLMs or VLMs in the data creation process. Rather than using models to generate training examples, this experiment algorithmically creates what we hope are realistic learning trajectories that simulate how spatial reasoning competency might develop over time.</p>
<p>I should note upfront that this approach has obvious limitations. We&rsquo;re essentially creating synthetic behaviors based on assumptions about how learning progresses, rather than capturing actual model failures and improvements. Whether this translates to meaningful training signal remains an open question that Part 2 of this series will attempt to address.</p>
<p><strong>Note:</strong> This is Part 1 of a two-part series focusing on the data generation methodology. Part 2 will cover model training and evaluation results, assuming they&rsquo;re worth sharing.</p>
<h2 id="the-challenge-with-current-approaches">The Challenge with Current Approaches<a hidden class="anchor" aria-hidden="true" href="#the-challenge-with-current-approaches">#</a></h2>
<p>Most spatial reasoning datasets suffer from significant limitations, though I should acknowledge that each approach has valid use cases. LLM-generated data requires expensive API costs for large-scale generation, introduces model bias in generated examples, and makes it difficult to control specific failure patterns (though it does capture more naturalistic language patterns than synthetic approaches). Vision-language models create dependencies on complex visual processing pipelines that are computationally expensive, but they do ground reasoning in actual visual understanding rather than tokenized abstractions. Manual annotation approaches are time-intensive and difficult to scale, but they provide the most authentic human reasoning patterns.</p>
<p>The approach described here attempts to sidestep these issues, though it likely introduces new problems of its own.</p>
<h2 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h2>
<h3 id="curriculum-framework-design">Curriculum Framework Design<a hidden class="anchor" aria-hidden="true" href="#curriculum-framework-design">#</a></h3>
<p>The framework implements five distinct difficulty levels, each representing what I hypothesize might be different stages of spatial reasoning competency. This is admittedly based on intuition rather than empirical study of how models actually learn spatial tasks.</p>
<p>Level 1 simulates initial learning with 5% prediction accuracy, 50% invalid move probability, and 5-step spatial errors from actual positions. The strategy focuses on random exploration with high error rates (though real novice behavior might be even more chaotic than this simplified model suggests). Level 2 introduces basic pattern recognition with 15% accuracy and directional bias emerging after turn 3. Level 3 develops intermediate planning capabilities using single waypoint-based navigation with 25% accuracy. Level 4 advances to dual waypoint planning with oscillation detection, achieving 50% accuracy with zero invalid moves in final turns. Level 5 represents expert performance with perfect A* pathfinding, 100% prediction accuracy, and zero spatial errors.</p>
<p>The progression assumes a roughly linear improvement in spatial capabilities, which may not reflect how models actually develop these skills. Real learning curves are likely more jagged, with sudden improvements and occasional regressions that this framework doesn&rsquo;t capture.</p>
<h3 id="data-generation-process">Data Generation Process<a hidden class="anchor" aria-hidden="true" href="#data-generation-process">#</a></h3>
<p>Each level generates conversational exchanges between human (maze state) and assistant (navigation response). The assistant&rsquo;s reasoning process includes spatial analysis of current position and target direction, multi-step move planning with appropriate error injection, level-appropriate motivation and strategy selection, move validity checking with position updates, and reflection generation analyzing prediction accuracy and learning progress.</p>
<h3 id="progressive-distribution">Progressive Distribution<a hidden class="anchor" aria-hidden="true" href="#progressive-distribution">#</a></h3>
<p>The curriculum uses a dynamic distribution across 100,000 training samples:</p>
<ul>
<li><strong>Samples 0-10k</strong>: 60% Level 1, 20% Level 2, declining higher levels</li>
<li><strong>Samples 10k-40k</strong>: Gradual shift toward intermediate levels</li>
<li><strong>Samples 40k-60k</strong>: Balanced distribution favoring Levels 2-3</li>
<li><strong>Samples 60k-80k</strong>: Emphasis on Levels 3-4 with emerging Level 5</li>
<li><strong>Samples 80k-100k</strong>: 60% Level 5, 40% Level 4, minimal lower levels</li>
</ul>
<h2 id="implementation-details">Implementation Details<a hidden class="anchor" aria-hidden="true" href="#implementation-details">#</a></h2>
<h3 id="maze-environment">Maze Environment<a hidden class="anchor" aria-hidden="true" href="#maze-environment">#</a></h3>
<p>The system uses a grid-based maze representation with:</p>
<ul>
<li>Randomized maze generation using depth-first search</li>
<li>Configurable sizes (4x4 to 8x8 grids)</li>
<li>Token-based representation: <code>&lt;|row-col|&gt;</code>, <code>&lt;|up_down_wall|&gt;</code>, <code>&lt;|origin|&gt;</code>, <code>&lt;|target|&gt;</code></li>
<li>Guaranteed solvability with optimal path lengths between 10-20 moves</li>
</ul>
<h3 id="behavior-modeling">Behavior Modeling<a hidden class="anchor" aria-hidden="true" href="#behavior-modeling">#</a></h3>
<p>Rather than capturing actual model outputs, the system generates hypothetical learning behaviors:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Example: Level 3 waypoint strategy</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_waypoint_moves</span>(self, maze, waypoint_metadata, current_target, turn):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> current_target <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;midpoint1&#39;</span>:
</span></span><span style="display:flex;"><span>        target_pos <span style="color:#f92672">=</span> waypoint_metadata[<span style="color:#e6db74">&#39;midpoint1&#39;</span>][<span style="color:#e6db74">&#39;position&#39;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        target_pos <span style="color:#f92672">=</span> maze<span style="color:#f92672">.</span>target
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculate directional bias toward current objective</span>
</span></span><span style="display:flex;"><span>    dr <span style="color:#f92672">=</span> target_pos[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> current_pos[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    dc <span style="color:#f92672">=</span> target_pos[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> current_pos[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Generate bias probabilities</span>
</span></span><span style="display:flex;"><span>    bias <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_calculate_directional_bias(dr, dc)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> maze<span style="color:#f92672">.</span>do_random_walk(
</span></span><span style="display:flex;"><span>        p<span style="color:#f92672">=</span>bias,
</span></span><span style="display:flex;"><span>        num_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
</span></span><span style="display:flex;"><span>        invalid_chance<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>invalid_chances[turn]
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><h3 id="conversation-structure">Conversation Structure<a hidden class="anchor" aria-hidden="true" href="#conversation-structure">#</a></h3>
<p>Each training sample contains:</p>
<ul>
<li><strong>Initial maze state</strong> (tokenized representation)</li>
<li><strong>Multi-turn conversation</strong> (up to 5 turns)</li>
<li><strong>Progressive reasoning</strong> (improving prediction accuracy)</li>
<li><strong>Strategic evolution</strong> (from random to optimal planning)</li>
<li><strong>Success tracking</strong> (target achievement metrics)</li>
</ul>
<h2 id="key-advantages-and-their-limitations">Key Advantages (and Their Limitations)<a hidden class="anchor" aria-hidden="true" href="#key-advantages-and-their-limitations">#</a></h2>
<p>This approach does offer some practical benefits over model-dependent generation. It eliminates the need for reinforcement learning training loops, policy optimization algorithms, large-scale model checkpointing, and extensive hyperparameter tuning. The synthetic generation allows precise control over error patterns and frequencies, strategic complexity progression, learning curve steepness, and failure mode representation. The framework supports unlimited data generation, diverse maze configurations, configurable difficulty progressions, and multi-domain adaptation potential.</p>
<p>However, these advantages come with significant trade-offs. The precision of control might actually be a weakness, afterall real learning is messier and less predictable than these neat progressions suggest. The unlimited data generation capability is only valuable if the generated data actually teaches useful patterns, which remains to be validated. Most importantly, this approach assumes we understand how spatial reasoning develops well enough to simulate it convincingly, which is probably overconfident.</p>
<h2 id="experimental-validation">Experimental Validation<a hidden class="anchor" aria-hidden="true" href="#experimental-validation">#</a></h2>
<h3 id="dataset-characteristics">Dataset Characteristics<a hidden class="anchor" aria-hidden="true" href="#dataset-characteristics">#</a></h3>
<p>Generated dataset contains:</p>
<ul>
<li>100,000 complete maze-solving conversations</li>
<li>Average 3.8 turns per conversation (range: 1-5)</li>
</ul>
<h3 id="behavioral-authenticity">Behavioral Authenticity<a hidden class="anchor" aria-hidden="true" href="#behavioral-authenticity">#</a></h3>
<p>The contrast between curriculum levels demonstrates realistic learning progression:</p>
<p><strong>Level 1 Example - Spatial Reasoning Breakdown:</strong></p>
<pre tabindex="0"><code>Prediction: &#34;I think I&#39;ll end up at &lt;0-0&gt;&#34;
Reality: Ends at &lt;1-1&gt;
Moves: &lt;|right|&gt; &lt;|left|&gt; &lt;|right|&gt; &lt;|left|&gt; &lt;|up|&gt; (oscillatory, inefficient)
Invalid attempts: Multiple failed &lt;|down|&gt; moves due to walls
Strategy: &#34;Let me map out the surrounding area&#34; (confused exploration)
</code></pre><p><strong>Level 5 Example - Expert Performance:</strong></p>
<pre tabindex="0"><code>Prediction: &#34;I think I&#39;ll end up at &lt;4-1&gt;&#34;  
Reality: &#34;Perfect! I&#39;m exactly at &lt;4-1&gt; - execution was flawless&#34;
Moves: Optimal A* pathfinding with dual waypoint strategy
Invalid attempts: Zero - all moves valid
Strategy: &#34;Looking ahead, I can see position &lt;4-0&gt; is only 3 moves from target&#34;
</code></pre><p>This progression from chaotic spatial confusion to perfect navigation demonstrates:</p>
<ul>
<li>Realistic error patterns decreasing over curriculum levels</li>
<li>Natural language variation reflecting competency changes</li>
<li>Strategic evolution from random exploration to optimal planning</li>
<li>Authentic learning trajectory simulation</li>
</ul>
<h2 id="what-would-probably-be-better">What Would Probably Be Better<a hidden class="anchor" aria-hidden="true" href="#what-would-probably-be-better">#</a></h2>
<p>A more rigorous approach would likely combine elements from multiple methodologies. Ideally, we&rsquo;d start with actual model behavior—training models on spatial tasks and carefully analyzing their failure modes, learning progressions, and breakthrough moments. This would provide empirical grounding for curriculum design rather than relying on assumptions.</p>
<p>Incorporating human learning data would add another valuable dimension. How do humans develop spatial reasoning skills? What kinds of mistakes do they make, and how do those mistakes change over time? Cognitive science research on spatial learning could inform more realistic error patterns and progression rates.</p>
<p>A hybrid approach might prove most effective: use LLMs to generate diverse initial examples, analyze real model failures to understand authentic confusion patterns, then use deterministic generation to scale up the most informative training scenarios. This would combine the naturalistic language of LLM generation, the authenticity of real model behavior, and the control and scalability of synthetic approaches.</p>
<p>For validation, we&rsquo;d want to compare not just final task performance but also the learning dynamics—do models trained on this synthetic curriculum develop spatial reasoning in similar ways to models trained through direct experience or human demonstration?</p>
<h2 id="future-directions">Future Directions<a hidden class="anchor" aria-hidden="true" href="#future-directions">#</a></h2>
<h3 id="dataset-and-model-release">Dataset and Model Release<a hidden class="anchor" aria-hidden="true" href="#dataset-and-model-release">#</a></h3>
<p>The complete dataset and models trained on this curriculum framework will be released in the coming months, including:</p>
<ul>
<li>Full 100,000-sample training dataset with all curriculum levels</li>
<li>Detailed methodology documentation and generation code</li>
<li>Evaluation benchmarks for spatial reasoning tasks</li>
</ul>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>This deterministic approach to spatial reasoning data generation represents an experiment in algorithmic curriculum design rather than a definitive solution. While it offers practical advantages over LLM-based or VLM-dependent methods through cost efficiency with no API calls or GPU-intensive model inference required, precise control over learning progression and error patterns, unlimited scalability with consistent quality, and complete transparency in understanding how each training example was created, these benefits may be offset by the artificial nature of the generated learning trajectories.</p>
<p>The framework demonstrates that it may be possible to create structured spatial reasoning training data through pure algorithmic generation, but whether this data actually improves model performance remains an empirical question. The approach might work well as a starting point or supplement to other training data, even if it doesn&rsquo;t fully replace more authentic alternatives.</p>
<p>The real test will be whether models trained on this curriculum develop generalizable spatial reasoning skills or simply learn to mimic the specific patterns encoded in the synthetic data. Part 2 will attempt to provide some preliminary answers, though a comprehensive evaluation would require much more extensive experimentation than a single project can provide.</p>
<p><strong>Coming Next:</strong> Part 2 of this series will cover training language models on this curriculum dataset and evaluating their spatial reasoning capabilities, assuming the results are interesting enough to warrant sharing.</p>
<hr>
<p><em>The complete dataset generation framework will be released regardless of training outcomes, as the methodology itself may be of interest even if the results are disappointing. For questions about the approach or early access to the dataset, please reach out through [amanpriyanshu.github.io].</em></p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://amanpriyanshu.github.io/blogs/tags/synthetic-data/">Synthetic Data</a></li>
      <li><a href="https://amanpriyanshu.github.io/blogs/tags/spatial-intelligence/">Spatial Intelligence</a></li>
      <li><a href="https://amanpriyanshu.github.io/blogs/tags/data-generation/">Data Generation</a></li>
      <li><a href="https://amanpriyanshu.github.io/blogs/tags/maze-navigation/">Maze Navigation</a></li>
      <li><a href="https://amanpriyanshu.github.io/blogs/tags/curriculum-learning/">Curriculum Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://amanpriyanshu.github.io/blogs/posts/2025/deepseek-reasoning-memotypes/">
    <span class="title">« Prev Page</span>
    <br>
    <span>DeepSeek&#39;s Reasoning Memotypes: Could Linguistic Patterns Be Replicating Through Synthetic Data?</span>
  </a>
  <a class="next" href="https://amanpriyanshu.github.io/blogs/posts/2024/dynamic-topic-modeling/">
    <span class="title">Next Page »</span>
    <br>
    <span>Teaching AI to Read and Group Like I Bookmark the Web: A Journey into Dynamic Topic Modeling</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial on twitter"
        href="https://twitter.com/intent/tweet/?text=Creating%202D%20Spatial%20Reasoning%20Data%20Without%20LLMs%20%2f%20VLMs%3a%20A%20Deterministic%20Curriculum%20Trial&amp;url=https%3a%2f%2famanpriyanshu.github.io%2fblogs%2fposts%2f2025%2fmazesolver-grpo-data%2f&amp;hashtags=SyntheticData%2cSpatialIntelligence%2cDataGeneration%2cMazeNavigation%2cCurriculumLearning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2famanpriyanshu.github.io%2fblogs%2fposts%2f2025%2fmazesolver-grpo-data%2f&amp;title=Creating%202D%20Spatial%20Reasoning%20Data%20Without%20LLMs%20%2f%20VLMs%3a%20A%20Deterministic%20Curriculum%20Trial&amp;summary=Creating%202D%20Spatial%20Reasoning%20Data%20Without%20LLMs%20%2f%20VLMs%3a%20A%20Deterministic%20Curriculum%20Trial&amp;source=https%3a%2f%2famanpriyanshu.github.io%2fblogs%2fposts%2f2025%2fmazesolver-grpo-data%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2famanpriyanshu.github.io%2fblogs%2fposts%2f2025%2fmazesolver-grpo-data%2f&title=Creating%202D%20Spatial%20Reasoning%20Data%20Without%20LLMs%20%2f%20VLMs%3a%20A%20Deterministic%20Curriculum%20Trial">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2famanpriyanshu.github.io%2fblogs%2fposts%2f2025%2fmazesolver-grpo-data%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial on whatsapp"
        href="https://api.whatsapp.com/send?text=Creating%202D%20Spatial%20Reasoning%20Data%20Without%20LLMs%20%2f%20VLMs%3a%20A%20Deterministic%20Curriculum%20Trial%20-%20https%3a%2f%2famanpriyanshu.github.io%2fblogs%2fposts%2f2025%2fmazesolver-grpo-data%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial on telegram"
        href="https://telegram.me/share/url?text=Creating%202D%20Spatial%20Reasoning%20Data%20Without%20LLMs%20%2f%20VLMs%3a%20A%20Deterministic%20Curriculum%20Trial&amp;url=https%3a%2f%2famanpriyanshu.github.io%2fblogs%2fposts%2f2025%2fmazesolver-grpo-data%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span></span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>




</html>

