<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Decoding AI&#39;s Evolution</title>
    <link>https://amanpriyanshu.github.io/blogs/posts/</link>
    <description>Recent content in Posts on Decoding AI&#39;s Evolution</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright> </copyright>
    <lastBuildDate>Tue, 12 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://amanpriyanshu.github.io/blogs/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepSeek&#39;s Reasoning Memotypes: Could Linguistic Patterns Be Replicating Through Synthetic Data?</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2025/deepseek-reasoning-memotypes/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2025/deepseek-reasoning-memotypes/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Analysis Code:&lt;/strong&gt; &lt;a href=&#34;https://gist.github.com/AmanPriyanshu/431df2cefe60386b0fc9cb66802b70d4&#34;&gt;GitHub Gist&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In biology, DNA provides the blueprint for how organisms develop and reproduce. In the realm of synthetic reasoning data, we observe a similar phenomenon: specific linguistic patterns that function as &amp;ldquo;reasoning memotypes&amp;rdquo;—self-replicating units of thought structure that propagate through synthetic data generation.&lt;/p&gt;
&lt;p&gt;Analysis of Nvidia&amp;rsquo;s Nemotron post-training dataset^[1], which contains over 30 million synthetic examples generated using DeepSeek R1^[2] and other reasoning models, reveals systematic linguistic patterns that appear with extraordinary frequency. These patterns function like genetic code for reasoning behavior, encoding not just what models think, but how they structure and express thought itself.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Creating 2D Spatial Reasoning Data Without LLMs / VLMs: A Deterministic Curriculum Trial</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2025/mazesolver-grpo-data/</link>
      <pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2025/mazesolver-grpo-data/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;
&lt;a href=&#34;https://github.com/AmanPriyanshu/Tiny-Maze-Mock-GRPO&#34;&gt;GitHub Repository&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/datasets/AmanPriyanshu/Tiny-Maze-Mock-GRPO&#34;&gt;Dataset Sample&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Spatial reasoning remains a significant challenge for language models, particularly in tasks requiring 2D navigation and visual-spatial understanding. Current approaches typically rely on either training large vision-language models (VLMs) on visual data or using language models to generate training examples through expensive API calls.&lt;/p&gt;
&lt;p&gt;This post explores what might be considered an unconventional (and possibly naive) approach: &lt;strong&gt;deterministic generation of spatial reasoning data&lt;/strong&gt; without requiring any LLMs or VLMs in the data creation process. Rather than using models to generate training examples, this experiment algorithmically creates what we hope are realistic learning trajectories that simulate how spatial reasoning competency might develop over time.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Teaching AI to Read and Group Like I Bookmark the Web: A Journey into Dynamic Topic Modeling</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/dynamic-topic-modeling/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/dynamic-topic-modeling/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;
&lt;a href=&#34;https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens&#34;&gt;Dataset on HuggingFace&lt;/a&gt;&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/AmanPriyanshu/blogs/refs/heads/main/content/posts/2024/Dynamic-Topic-Modeling/images/dynamic-topics-banner.png&#34; alt=&#34;Dynamic Topic Modeling&#34; style=&#34;width: 96%;&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;the-topic-modeling-challenge&#34;&gt;The Topic Modeling Challenge&lt;/h2&gt;
&lt;p&gt;You know that feeling when you have 50 browser tabs open, and you&amp;rsquo;re desperately trying to organize them into bookmark folders? &amp;ldquo;ML Papers To Read,&amp;rdquo; &amp;ldquo;Funny Cat Videos,&amp;rdquo; &amp;ldquo;Recipes I&amp;rsquo;ll Never Make&amp;rdquo;&amp;hellip; We all have our system. And apparently, it&amp;rsquo;s such a universal problem that every tech company is launching their own solution - Arc Browser with its &amp;ldquo;Spaces,&amp;rdquo; Chrome with its tab groups, and about 500 extensions promising to color-code your digital hoarding habits into submission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Contra-Topic-bottleneck-t5: Efficient Topic Extraction Without the Computational Overhead</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/contra-topic/</link>
      <pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/contra-topic/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;
&lt;a href=&#34;https://huggingface.co/AmanPriyanshu/Contra-Topic-bottleneck-t5-large&#34;&gt;Model on HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/drive/1_SuTiL3QS-PUYjSrugqqD5mQlMv8Hbfc?usp=sharing&#34;&gt;Interactive Demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When it comes to topic extraction, the AI world seems fixated on massive models and expensive compute. But what if there was a simpler way? 🤔&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/AmanPriyanshu/blogs/refs/heads/main/content/posts/2024/Contra-Topic/images/contra-topic-banner.png&#34; alt=&#34;Topic Modeling&#34; style=&#34;width: 96%;&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;the-genesis-simplicity-through-linear-transformation&#34;&gt;The Genesis: Simplicity Through Linear Transformation&lt;/h2&gt;
&lt;p&gt;Picture this: There I was, looking for an open-source solution to extract topics from text at scale. The available options were either massive language models or complex fine-tuning pipelines. That&amp;rsquo;s when it hit me – what if we could leverage the semantic structure of existing embeddings with just a linear transformation?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LinearCosine: When AI Researchers Decided Multiplication was Too Mainstream</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/linear-cosine/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/linear-cosine/</guid>
      <description>&lt;p&gt;Hey there, optimization seekers and efficiency enthusiasts! 📊🧮 Today, we&amp;rsquo;re diving into a world where even basic arithmetic operations are up for debate. Buckle up as we explore &lt;a href=&#34;https://amanpriyanshu.github.io/LinearCosine/&#34;&gt;LinearCosine, an experiment that asks: &amp;ldquo;Do we really need multiplication for AI?&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick Links to skip the talk:&lt;/strong&gt;
&lt;a href=&#34;https://amanpriyanshu.github.io/LinearCosine/&#34;&gt;Project Website - Linear Cosine&lt;/a&gt; | &lt;a href=&#34;https://github.com/AmanPriyanshu/LinearCosine&#34;&gt;GitHub Repo&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2410.00907&#34;&gt;Original Paper&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-paper-that-started-it-all&#34;&gt;The Paper That Started It All&lt;/h2&gt;
&lt;p&gt;During my fall break, while I was supposed to be relaxing, my roommate Yash Maurya forwarded me a fascinating paper by Hongyin Luo and Wei Sun titled &amp;ldquo;&lt;a href=&#34;https://arxiv.org/abs/2410.00907&#34;&gt;Addition is All You Need for Energy-efficient Language Models&lt;/a&gt;&amp;rdquo;. I was immediately intrigued by their approach to modify one of the core fundamental computations in AI, multiplication. This project builds upon my previous work on &lt;a href=&#34;https://amanpriyanshu.github.io/blogs/posts/2024/startup-linguistic-trees/&#34;&gt;in-browser vanilla js semantic search, such as YC-Dendrolinguistics&lt;/a&gt;, where I implemented a &lt;a href=&#34;https://amanpriyanshu.github.io/YC-Dendrolinguistics/&#34;&gt;cosine similarity-based information retrieval system for YC startups&lt;/a&gt;. LinearCosine takes this a step further by exploring ways to make these fundamental calculations more energy-efficient.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AdaptKeyBERT: Stumbling Through Two Years of Keyword Extraction</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/adaptkeybert/</link>
      <pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/adaptkeybert/</guid>
      <description>&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://amanpriyanshu.github.io/AdaptKeyBERT/images/adaptkeybert_revisited.png&#34; alt=&#34;Running Demo 1&#34; style=&#34;width: 90%;&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Quick links (in case you want to skip my ramblings):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/adaptkeybert/&#34;&gt;PyPI Package&lt;/a&gt;
&lt;a href=&#34;https://github.com/AmanPriyanshu/AdaptKeyBERT&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Alright, gather &amp;lsquo;round, word enthusiasts and syntax sorcerers! 🧙‍♂️📚 Remember that time you tried to explain machine learning to your grandma and ended up comparing neural networks to her knitting patterns? Well, buckle up, because we&amp;rsquo;re about to dive into a similar realm of &amp;ldquo;What was I thinking?&amp;rdquo; – the saga of AdaptKeyBERT.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>API-LLM-Hub: Simplifying LLM-API integration for Static Pages</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/api-llm-hub/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/api-llm-hub/</guid>
      <description>&lt;p&gt;Hey there, fellow code enthusiasts and AI wranglers! 🖐️🤖 You know that feeling when you&amp;rsquo;re knee-deep in a project, trying to get multiple AI models to play nice in your browser? Yeah, I&amp;rsquo;ve been there. Cue the frustrated sighs, the endless searches over GitHub issues 😢, and the &amp;ldquo;why-isn&amp;rsquo;t-this-working&amp;rdquo; hair-pulling sessions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://amanpriyanshu.github.io/API-LLM-Hub/&#34;&gt;&lt;strong&gt;LINK-TO-PACKAGE&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;just in case you wanna skip the deets&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After one too many nights wrestling with backends, CORS issues, and the general chaos of integrating various AI APIs, I decided enough was enough. There had to be a simpler way, right? Something that didn&amp;rsquo;t require installing a bunch of npm builds, juggling APIs, or managing a backend server farm just to get a chatbot running on a static page.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>YC-Dendrolinguistics: Planting Linguistic Trees in the Startup Forest</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/startup-linguistic-trees/</link>
      <pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/startup-linguistic-trees/</guid>
      <description>&lt;p&gt;Hey there, fellow AI adventurers and startup enthusiasts! 🌳🚀 Today, I&amp;rsquo;m excited to give you a peek into my latest passion project: YC-Dendrolinguistics. Buckle up as we embark on a journey through the linguistic forests of Y-Combinator pitches!&lt;/p&gt;
&lt;h2 id=&#34;the-seed-of-an-idea&#34;&gt;The Seed of an Idea&lt;/h2&gt;
&lt;p&gt;Picture this: It&amp;rsquo;s 2 AM, I&amp;rsquo;m knee-deep in YC application videos, and suddenly it hits me – what if startup pitches are like trees? 🤔 Each word a branch, each phrase a limb, growing into this complex organism we call a pitch. That&amp;rsquo;s when YC-Dendrolinguistics was born, my wild attempt to map the DNA of startup communication.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Synaptic Sparks: Why I&#39;m Wiring My Thoughts into a Neural Blogosphere</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/synaptic-sparks/</link>
      <pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/synaptic-sparks/</guid>
      <description>&lt;p&gt;Hey there, fellow AI enthusiasts and curious minds! 🧠🤖 Today, I just want to document what&amp;rsquo;s leading to this new adventure in regular blogging.&lt;/p&gt;
&lt;h2 id=&#34;the-knowledge-synapse&#34;&gt;The Knowledge Synapse&lt;/h2&gt;
&lt;p&gt;Picture me back in 2019, a wide-eyed novice bouncing around the vast landscape of machine learning. I was devouring every GitHub gist, Medium post, and arXiv paper I could find, growing and learning at a dizzying pace. Fast forward to today, and it feels like I&amp;rsquo;ve stepped into an alternate universe. So much of that knowledge that shaped me is now locked behind paywalls, long arduous youtube playlists, feeling almost alien to the very person who spent countless hours absorbing it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FRACTURED-SORRY-Bench: Unraveling AI Safety through Decomposing Malicious Intents</title>
      <link>https://amanpriyanshu.github.io/blogs/posts/2024/fractured-sorry-bench/</link>
      <pubDate>Wed, 28 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://amanpriyanshu.github.io/blogs/posts/2024/fractured-sorry-bench/</guid>
      <description>&lt;p&gt;Hello, fellow AI enthusiasts! 🤖 Today, I wanted to dive into the FRACTURED-SORRY-Bench framework and dataset we just released. Check out the &lt;a href=&#34;https://huggingface.co/datasets/AmanPriyanshu/FRACTURED-SORRY-Bench&#34;&gt;dataset&lt;/a&gt;, &lt;a href=&#34;https://amanpriyanshu.github.io/FRACTURED-SORRY-Bench/&#34;&gt;website&lt;/a&gt;, and &lt;a href=&#34;https://github.com/AmanPriyanshu/FRACTURED-SORRY-Bench/&#34;&gt;github&lt;/a&gt; for the dataset!&lt;/p&gt;
&lt;h2 id=&#34;the-fractured-sorry-saga-a-tale-of-adaptation-and-decomposition&#34;&gt;The FRACTURED-SORRY Saga: A Tale of Adaptation and Decomposition&lt;/h2&gt;
&lt;p&gt;Picture this: you&amp;rsquo;re wandering through the lush collection of prompt-injection and llm-red-teaming papers, marveling at some of the weird and some of the crazier attack mechanisms that have been released recently. When suddenly, you realize that there aren&amp;rsquo;t many Proof-of-Concept resources for multi-shot red-teaming. That&amp;rsquo;s essentially the story behind creating FRACTURED-SORRY-Bench.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
