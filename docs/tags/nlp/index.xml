<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Decoding AI&#39;s Evolution</title>
    <link>http://localhost:1313/blogs/tags/nlp/</link>
    <description>Recent content in NLP on Decoding AI&#39;s Evolution</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright> </copyright>
    <lastBuildDate>Mon, 11 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/blogs/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Teaching AI to Read and Group Like I Bookmark the Web: A Journey into Dynamic Topic Modeling</title>
      <link>http://localhost:1313/blogs/posts/2024/dynamic-topic-modeling/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blogs/posts/2024/dynamic-topic-modeling/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;
&lt;a href=&#34;https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens&#34;&gt;Dataset on HuggingFace&lt;/a&gt;&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/AmanPriyanshu/blogs/refs/heads/main/content/posts/2024/Dynamic-Topic-Modeling/images/dynamic-topics-banner.png&#34; alt=&#34;Dynamic Topic Modeling&#34; style=&#34;width: 96%;&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;the-topic-modeling-challenge&#34;&gt;The Topic Modeling Challenge&lt;/h2&gt;
&lt;p&gt;You know that feeling when you have 50 browser tabs open, and you&amp;rsquo;re desperately trying to organize them into bookmark folders? &amp;ldquo;ML Papers To Read,&amp;rdquo; &amp;ldquo;Funny Cat Videos,&amp;rdquo; &amp;ldquo;Recipes I&amp;rsquo;ll Never Make&amp;rdquo;&amp;hellip; We all have our system. And apparently, it&amp;rsquo;s such a universal problem that every tech company is launching their own solution - Arc Browser with its &amp;ldquo;Spaces,&amp;rdquo; Chrome with its tab groups, and about 500 extensions promising to color-code your digital hoarding habits into submission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Contra-Topic-bottleneck-t5: Efficient Topic Extraction Without the Computational Overhead</title>
      <link>http://localhost:1313/blogs/posts/2024/contra-topic/</link>
      <pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blogs/posts/2024/contra-topic/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;
&lt;a href=&#34;https://huggingface.co/AmanPriyanshu/Contra-Topic-bottleneck-t5-large&#34;&gt;Model on HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/drive/1_SuTiL3QS-PUYjSrugqqD5mQlMv8Hbfc?usp=sharing&#34;&gt;Interactive Demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When it comes to topic extraction, the AI world seems fixated on massive models and expensive compute. But what if there was a simpler way? ü§î&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/AmanPriyanshu/blogs/refs/heads/main/content/posts/2024/Contra-Topic/images/contra-topic-banner.png&#34; alt=&#34;Topic Modeling&#34; style=&#34;width: 96%;&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;the-genesis-simplicity-through-linear-transformation&#34;&gt;The Genesis: Simplicity Through Linear Transformation&lt;/h2&gt;
&lt;p&gt;Picture this: There I was, looking for an open-source solution to extract topics from text at scale. The available options were either massive language models or complex fine-tuning pipelines. That&amp;rsquo;s when it hit me ‚Äì what if we could leverage the semantic structure of existing embeddings with just a linear transformation?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AdaptKeyBERT: Stumbling Through Two Years of Keyword Extraction</title>
      <link>http://localhost:1313/blogs/posts/2024/adaptkeybert/</link>
      <pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blogs/posts/2024/adaptkeybert/</guid>
      <description>&lt;div style=&#34;display: flex; justify-content: space-between;&#34;&gt;
  &lt;img src=&#34;https://amanpriyanshu.github.io/AdaptKeyBERT/images/adaptkeybert_revisited.png&#34; alt=&#34;Running Demo 1&#34; style=&#34;width: 90%;&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Quick links (in case you want to skip my ramblings):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/adaptkeybert/&#34;&gt;PyPI Package&lt;/a&gt;
&lt;a href=&#34;https://github.com/AmanPriyanshu/AdaptKeyBERT&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Alright, gather &amp;lsquo;round, word enthusiasts and syntax sorcerers! üßô‚Äç‚ôÇÔ∏èüìö Remember that time you tried to explain machine learning to your grandma and ended up comparing neural networks to her knitting patterns? Well, buckle up, because we&amp;rsquo;re about to dive into a similar realm of &amp;ldquo;What was I thinking?&amp;rdquo; ‚Äì the saga of AdaptKeyBERT.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>API-LLM-Hub: Simplifying LLM-API integration for Static Pages</title>
      <link>http://localhost:1313/blogs/posts/2024/api-llm-hub/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blogs/posts/2024/api-llm-hub/</guid>
      <description>&lt;p&gt;Hey there, fellow code enthusiasts and AI wranglers! üñêÔ∏èü§ñ You know that feeling when you&amp;rsquo;re knee-deep in a project, trying to get multiple AI models to play nice in your browser? Yeah, I&amp;rsquo;ve been there. Cue the frustrated sighs, the endless searches over GitHub issues üò¢, and the &amp;ldquo;why-isn&amp;rsquo;t-this-working&amp;rdquo; hair-pulling sessions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://amanpriyanshu.github.io/API-LLM-Hub/&#34;&gt;&lt;strong&gt;LINK-TO-PACKAGE&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;just in case you wanna skip the deets&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After one too many nights wrestling with backends, CORS issues, and the general chaos of integrating various AI APIs, I decided enough was enough. There had to be a simpler way, right? Something that didn&amp;rsquo;t require installing a bunch of npm builds, juggling APIs, or managing a backend server farm just to get a chatbot running on a static page.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FRACTURED-SORRY-Bench: Unraveling AI Safety through Decomposing Malicious Intents</title>
      <link>http://localhost:1313/blogs/posts/2024/fractured-sorry-bench/</link>
      <pubDate>Wed, 28 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blogs/posts/2024/fractured-sorry-bench/</guid>
      <description>&lt;p&gt;Hello, fellow AI enthusiasts! ü§ñ Today, I wanted to dive into the FRACTURED-SORRY-Bench framework and dataset we just released. Check out the &lt;a href=&#34;https://huggingface.co/datasets/AmanPriyanshu/FRACTURED-SORRY-Bench&#34;&gt;dataset&lt;/a&gt;, &lt;a href=&#34;https://amanpriyanshu.github.io/FRACTURED-SORRY-Bench/&#34;&gt;website&lt;/a&gt;, and &lt;a href=&#34;https://github.com/AmanPriyanshu/FRACTURED-SORRY-Bench/&#34;&gt;github&lt;/a&gt; for the dataset!&lt;/p&gt;
&lt;h2 id=&#34;the-fractured-sorry-saga-a-tale-of-adaptation-and-decomposition&#34;&gt;The FRACTURED-SORRY Saga: A Tale of Adaptation and Decomposition&lt;/h2&gt;
&lt;p&gt;Picture this: you&amp;rsquo;re wandering through the lush collection of prompt-injection and llm-red-teaming papers, marveling at some of the weird and some of the crazier attack mechanisms that have been released recently. When suddenly, you realize that there aren&amp;rsquo;t many Proof-of-Concept resources for multi-shot red-teaming. That&amp;rsquo;s essentially the story behind creating FRACTURED-SORRY-Bench.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
